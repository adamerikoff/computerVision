{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Multiclass regression Implementation\n",
    "\n",
    "\n",
    "## 1. Information\n",
    "Logistic regression is a machine learning algorithm for binary classification problems.\n",
    "\n",
    "Logistic regression is similar to linear regression. We’re still dealing with a line equation for making predictions. The results are passed in a Sigmoid activation function to convert real values to probabilities.\n",
    "\n",
    "The probability tells you the chance of the instance belonging to a positive class. These probabilities are then turned to actual classes based on a threshold value.\n",
    "\n",
    "#### 1.2 Choses mathématiques\n",
    "We’re still dealing with a line equation:\n",
    "$$\\hat{y}=wx+b$$\n",
    "The output of the line equation is passed through a Sigmoid (Logistic) function\n",
    "$$S(x)=\\frac{1}{1+e^{-x}}$$\n",
    "The purpose of a sigmoid function is to take any real value and map it to a probability — value between zero and one.\n",
    "\n",
    "As a cost function, we’ll use a Binary Cross Entropy function, shown in the following formula:\n",
    "$$BCE = -\\frac{1}{n}\\sum_{i}^n y_i log\\hat{y}+(1+y_i)log(1-\\hat{y})$$\n",
    "\n",
    "We will need to use this cost function in the optimization process to update weights and bias iteratively. \n",
    "$$\\partial_w = \\frac{1}{n}\\sum_{i}^n2x_i(\\hat{y}-y_i)$$\n",
    "$$\\partial_b = \\frac{1}{n}\\sum_{i}^n2(\\hat{y}-y_i)$$\n",
    "\n",
    "Gradient descent update rules\n",
    "$$w=w-\\alpha \\partial_w$$\n",
    "$$b=b-\\alpha \\partial_b$$\n",
    "\n",
    "#### 1.3 NumPy Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, lr=0.1, iterations=1000):\n",
    "        self.lr = lr\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        #gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = X @ self.weights + self.bias\n",
    "            sigmoid_output = self._sigmoid(linear_model)\n",
    "            \n",
    "            dw = (X.T * (sigmoid_output - y)).T.mean(axis=0)\n",
    "            db = (sigmoid_output - y).mean(axis=0)\n",
    "\n",
    "            self.weights = self.weights - self.lr * dw\n",
    "            self.bias = self.bias - self.lr * db \n",
    "\n",
    "    def predict(self,X):\n",
    "        linear_model = np.dot(X,self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        return y_predicted\n",
    "  \n",
    "    def _sigmoid(self,x):\n",
    "        return(1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassClassification:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for y_i in np.unique(y):\n",
    "            # y_i - positive class for now\n",
    "            # All other classes except y_i are negative\n",
    "\n",
    "            # Choose x where y is positive class\n",
    "            x_true = X[y == y_i]\n",
    "            # Choose x where y is negative class\n",
    "            x_false = X[y != y_i]\n",
    "            # Concatanate\n",
    "            x_true_false = np.vstack((x_true, x_false))\n",
    "\n",
    "            # Set y to 1 where it is positive class\n",
    "            y_true = np.ones(x_true.shape[0])\n",
    "            # Set y to 0 where it is negative class\n",
    "            y_false = np.zeros(x_false.shape[0])\n",
    "            # Concatanate\n",
    "            y_true_false = np.hstack((y_true, y_false))\n",
    "\n",
    "            # Fit model and append to models list\n",
    "            model = LogisticRegression()\n",
    "            model.fit(x_true_false, y_true_false)\n",
    "            self.models.append([y_i, model])\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [[label, model.predict(X)] for label, model in self.models]\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            max_label = None\n",
    "            max_prob = -10**5\n",
    "            for j in range(len(y_pred)):\n",
    "                prob = y_pred[j][1][i]\n",
    "                if prob > max_prob:\n",
    "                    max_label = y_pred[j][0]\n",
    "                    max_prob = prob\n",
    "            output.append(max_label)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
