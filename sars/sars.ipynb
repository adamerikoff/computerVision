{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **State–action–reward–state–action (SARSA)**\n",
    "The __SARSA__ *(State-Action-Reward-State-Action)* algorithm is a reinforcement learning algorithm used to solve Markov Decision Processes (MDPs) in the context of sequential decision-making. SARSA is a member of the family of temporal difference (TD) learning methods and is particularly useful for training agents in environments with discrete states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Components of SARSA**\n",
    "- **State (S):** In a reinforcement learning problem, the environment is divided into states (or observations), which represent the situations or configurations that the agent can encounter.<br><br>\n",
    "- **Action (A):** At each state, the agent can take one of several possible actions. Actions represent the choices made by the agent to interact with the environment.<br><br>\n",
    "- **Reward (R):** After taking an action in a specific state, the agent receives a reward from the environment. Rewards provide feedback to the agent about the desirability of its actions.<br><br>\n",
    "- **Policy (π):** The policy defines the agent's strategy or behavior, specifying which action to take in each state. SARSA is an on-policy algorithm, meaning it learns and updates the policy it uses to interact with the environment.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key Concepts of SARSA**\n",
    "- **Q-Values (Action-Value Function):** SARSA aims to estimate the Q-values, denoted as Q(s, a), which represent the expected cumulative reward that can be obtained by starting in state $s$, taking action $a$, and following a specific policy $\\pi$ thereafter. The goal is to find the optimal Q-values that maximize the expected return.<br><br>\n",
    "- **Q-Table:** SARSA maintains a Q-table (or Q-function), which is a data structure that stores Q-values for each state-action pair. Initially, these values are typically initialized randomly.<br><br>\n",
    "- **Exploration vs. Exploitation:** SARSA balances exploration (trying different actions to discover the environment) and exploitation (choosing actions that are believed to be the best based on current Q-value estimates). This balance is controlled by an exploration strategy, often based on $\\varepsilon$-greedy exploration, where with probability $\\varepsilon$, the agent chooses a random action, and with probability $1-\\varepsilon$, it chooses the action with the highest estimated Q-value.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SARSA Algorithm**\n",
    "The SARSA algorithm updates Q-values iteratively through experience gained while interacting with the environment. The update rule for SARSA is as follows:\n",
    "$$\n",
    "    Q(s,a) \\rightarrow Q(s,a) + \\alpha [R+\\gamma Q(s',a') - Q(s,a)]\n",
    "$$\n",
    "- **Q(s,a)** is the current $Q$-value estimate for state $s$ and action $a$.<br><br>\n",
    "- **$\\alpha$** is the learning rate, which controls the size of the $Q$-value updates.<br><br>\n",
    "- **R** is the immediate reward received after taking action $a$ in state $s$.<br><br>\n",
    "- **$\\gamma$** is the discount factor, representing the importance of future rewards.<br><br>\n",
    "- **$s'$** is the next state reached after taking action $a$ in state $s$.<br><br>\n",
    "- **$a'$** is the action selected in state $s'$ according to the current policy.<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
